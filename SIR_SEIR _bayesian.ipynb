{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bayesian modelling with SIR/SEIR</h1>\n",
    "<p>We used the covid19_inference library provided at </p>\n",
    "<p>We rely on pymc3 library to sample from our model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I#Import the necessary libraries\n",
    "import os, sys\n",
    "\n",
    "import cachetools.func\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json,urllib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>The following cells are responsible for crawling the data and processing it</big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----      \n",
    "#-----\n",
    "def fix_region_name(df, pairs = [[\"Mainland China\", \"China\"]]):\n",
    "  # fix region names\n",
    "    for p in pairs:\n",
    "        df['Country/Region'] = df['Country/Region'].str.replace(p[0],p[1])\n",
    "    return df\n",
    "\n",
    "#-----\n",
    "def merge_df_data(df1,df2):\n",
    "    return pd.merge(df1, df2,how='left' ,on=['Province/State','Country/Region'])\n",
    "\n",
    "#-----\n",
    "def str_add_func(*args):      \n",
    "    out = []\n",
    "    for x in args:\n",
    "        if isinstance(x,str):\n",
    "            out.append(x)\n",
    "  \n",
    "    return '_'.join(out)\n",
    "\n",
    "\n",
    "class covid_data():\n",
    "    '''\n",
    "      Python class to obtain global COVID19 data from \n",
    "      John Hopkins GIT repository. This data is updated daily, \n",
    "      and the most upto date information available on the web.  \n",
    "    '''\n",
    "    def __init__(self,**kwargs):\n",
    "        nrow = kwargs.get('nrow',None)\n",
    "        self.confirmed, self.dead, self.recovered = self.get_csseg_data(nrow=nrow)\n",
    "    @staticmethod\n",
    "    def create_ts(df):\n",
    "        ts=df\n",
    "        columns = ts['region']\n",
    "        ts=ts.drop(['Province/State', \n",
    "                    'Country/Region',\n",
    "                    'Lat', \n",
    "                    'Long',\n",
    "                    'Population'], \n",
    "                   axis=1).set_index('region').T    \n",
    "\n",
    "        ts.columns = columns \n",
    "        ts=ts.fillna(0)\n",
    "        #\n",
    "        ts.index.name = 'Date'\n",
    "        return ts\n",
    "\n",
    "    def search_agg(self, name,col='Country/Region',ts=True):\n",
    "    \n",
    "        if not isinstance(name,list):\n",
    "            name = [name]\n",
    "\n",
    "        out = {}\n",
    "        for k,v in {'confirmed':self.confirmed,\n",
    "                    'dead':self.dead,\n",
    "                    'recovered':self.recovered}.items():\n",
    "\n",
    "        #pd.columns(columns=)\n",
    "            df_list= []     \n",
    "            for n in name:\n",
    "                df = v[v[col]==n].set_index(col).filter(regex='/20')\n",
    "                df_list.append(df.sum(axis=0))\n",
    "\n",
    "            df = pd.concat(df_list,axis=1, sort=False)\n",
    "            df.columns = name\n",
    "            out[k] = df\n",
    "\n",
    "        # if ts:                \n",
    "        #   out[k] = self.create_ts(df)\n",
    "        # else:\n",
    "        #   out[k] = df.T\n",
    "\n",
    "        return out\n",
    "\n",
    "    def search(self, name,col='Country/Region',ts=True):\n",
    "        if not isinstance(name,list):\n",
    "            name = [name]\n",
    "        out = {}\n",
    "        for k,v in {'confirmed':self.confirmed,\n",
    "                    'dead':self.dead,\n",
    "                    'recovered':self.recovered}.items():\n",
    "            if ts:                \n",
    "                out[k] = self.create_ts(v[v[col].map(lambda x: x in name)])\n",
    "            else:\n",
    "                out[k] = v[v[col] in name].T\n",
    "        return out\n",
    "\n",
    "    @cachetools.func.ttl_cache(maxsize=128, ttl=24 * 60)\n",
    "    def get_csseg_data(self, nrow=None):\n",
    "    \n",
    "        url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master'\n",
    "        path = f'{url}/csse_covid_19_data/csse_covid_19_time_series' \n",
    "\n",
    "        # \n",
    "    \n",
    "        url = f'{path}/time_series_covid19_confirmed_global.csv'\n",
    "        confirmed = fix_region_name(pd.read_csv(url, nrows=nrow, error_bad_lines=False))\n",
    "        #\n",
    "        url = f'{path}/time_series_covid19_deaths_global.csv'\n",
    "        dead = fix_region_name(pd.read_csv(url, nrows=nrow, error_bad_lines=False))\n",
    "        #\n",
    "        url = f'{path}/time_series_covid19_recovered_global.csv'\n",
    "    \n",
    "        recovered = fix_region_name(pd.read_csv(url, nrows=nrow, error_bad_lines=False))\n",
    "        print(confirmed.head())\n",
    "        #\n",
    "        return confirmed, dead, recovered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the covid_inference library\n",
    "try:\n",
    "    import covid19_inference as cov19\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(\"../../\")\n",
    "    import covid19_inference as cov19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = covid_data()\n",
    "cd.confirmed.head()\n",
    "countries = ['Senegal']\n",
    "mm = cd.search_agg(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, ctype in enumerate(['confirmed', 'dead', 'recovered']):\n",
    "    df = mm[ctype].stack().reset_index()\n",
    "    #print(df.head())\n",
    "    df = df.rename(columns={'level_0':'date','level_1':'country',0:ctype})     \n",
    "    if ix==0:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        dfall = df\n",
    "    else:\n",
    "        dfall[ctype] = df[ctype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall=dfall.loc[dfall['confirmed']>=100] # ensure we have data with community spread\n",
    "\n",
    "dfall.drop(['country','recovered'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks if there are no missing date\n",
    "assert len(dfall)==(datetime.datetime(2020,8,7)-datetime.datetime(2020,3,26)).days+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the length of forecast\n",
    "training_date=datetime.datetime(2020,7,25)\n",
    "forecast_lenght=(datetime.datetime(2020,8,10)-training_date).days\n",
    "forecast_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=dfall.loc[dfall['date']<training_date].set_index(['date'])\n",
    "validation_data=dfall.loc[dfall['date']>training_date].set_index(['date'])\n",
    "new_cases=training_data['confirmed']\n",
    "cum_cases=training_data['confirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the parameter that is used in modelling\n",
    "params_model = dict(\n",
    "    new_cases_obs=new_cases,\n",
    "    data_begin=datetime.datetime(2020,3,26),\n",
    "    fcast_len=forecast_lenght,\n",
    "    diff_data_sim=16,\n",
    "    N_population=16e6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of change points- Non pharmceutical intervention\n",
    "change_points = [\n",
    "    #religious activities was put on hold\n",
    "    dict(\n",
    "        pr_mean_date_transient=datetime.datetime(2020, 3, 10),\n",
    "        # account for new implementation where transients_day is centered, not begin\n",
    "        pr_median_transient_len=3,\n",
    "        pr_sigma_transient_len=0.3,\n",
    "        pr_sigma_date_transient=3,\n",
    "        pr_median_lambda=0.2,\n",
    "        pr_sigma_lambda=0.5,\n",
    "    ),\n",
    "    #Travel restrictions\n",
    "    dict(\n",
    "        pr_mean_date_transient=datetime.datetime(2020, 3, 15),\n",
    "        # account for new implementation where transients_day is centered, not begin\n",
    "        pr_median_transient_len=3,\n",
    "        pr_sigma_transient_len=0.3,\n",
    "        pr_sigma_date_transient=3,\n",
    "        pr_median_lambda=0.2,\n",
    "        pr_sigma_lambda=0.5,\n",
    "    ),\n",
    "    # State of emergency\n",
    "    dict(\n",
    "        pr_mean_date_transient=datetime.datetime(2020, 3, 23),\n",
    "        # account for new implementation where transients_day is centered, not begin\n",
    "        pr_median_transient_len=3,\n",
    "        pr_sigma_transient_len=0.3,\n",
    "        pr_sigma_date_transient=3,\n",
    "        pr_median_lambda=0.2,\n",
    "        pr_sigma_lambda=0.5,\n",
    "    ),\n",
    "    # strong distancing\n",
    "    dict(\n",
    "        pr_mean_date_transient=datetime.datetime(2020, 3, 31)\n",
    "        + datetime.timedelta(days=1.5),\n",
    "        pr_median_transient_len=3,\n",
    "        pr_sigma_transient_len=0.3,\n",
    "        pr_sigma_date_transient=1,\n",
    "        pr_median_lambda=1 / 8,\n",
    "        pr_sigma_lambda=0.5,\n",
    "    ),\n",
    "    # the President lifted the state of emergency and curfew\n",
    "    dict(\n",
    "        pr_mean_date_transient=datetime.datetime(2020, 6, 30),\n",
    "        # account for new implementation where transients_day is centered, not begin\n",
    "        pr_median_transient_len=3,\n",
    "        pr_sigma_transient_len=0.3,\n",
    "        pr_sigma_date_transient=3,\n",
    "        pr_median_lambda=0.2,\n",
    "        pr_sigma_lambda=0.5,\n",
    "    ),\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of the prior for the delay in case reporting, we assumed 8 days\n",
    "pr_delay = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cov19.Cov19Model(**params_model) as this_model:\n",
    "    # Create the an array of the time dependent infection rate lambda\n",
    "    lambda_t_log = cov19.model.lambda_t_with_sigmoids(\n",
    "        pr_median_lambda_0=0.4,\n",
    "        pr_sigma_lambda_0=0.5,\n",
    "        change_points_list=change_points,\n",
    "        name_lambda_t=\"lambda_t\",  # Name for the variable in the trace\n",
    "    )\n",
    "\n",
    "    # Adds the recovery rate mu to the model as a random variable\n",
    "    mu = pm.Lognormal(name=\"mu\", mu=np.log(1 / 8), sigma=0.2,testval=10)\n",
    "\n",
    "    # This builds a decorrelated prior for I_begin for faster inference. It is not\n",
    "    # necessary to use it, one can simply remove it and use the default argument for\n",
    "    # pr_I_begin in cov19.model.SIR\n",
    "    prior_I = cov19.model.uncorrelated_prior_I(\n",
    "        lambda_t_log=lambda_t_log, mu=mu, pr_median_delay=pr_delay\n",
    "    )\n",
    "\n",
    "    # Use lambda_t_log and mu as parameters for the SIR model.\n",
    "    # The SIR model generates the inferred new daily cases.\n",
    "    new_cases = cov19.model.SIR(lambda_t_log=lambda_t_log, mu=mu, pr_I_begin=prior_I)\n",
    "\n",
    "    # Delay the cases by a lognormal reporting delay and add them as a trace variable\n",
    "    new_cases = cov19.model.delay_cases(\n",
    "        cases=new_cases,\n",
    "        name_cases=\"delayed_cases\",\n",
    "        pr_mean_of_median=pr_delay,\n",
    "        pr_median_of_width=0.1,\n",
    "    )\n",
    "\n",
    "    # Modulate the inferred cases by a abs(sin(x)) function, to account for weekend effects\n",
    "    # Also adds the \"new_cases\" variable to the trace that has all model features.\n",
    "    new_cases = cov19.model.week_modulation(cases=new_cases, name_cases=\"new_cases\")\n",
    "\n",
    "    # Define the likelihood, uses the new_cases_obs set as model parameter\n",
    "    cov19.model.student_t_likelihood(cases=new_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pm.sample(model=this_model, tune=2000, draws=2000) #sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = cov19.plot.timeseries_overview(this_model, trace, offset=cum_cases[0])\n",
    "for ax in axes:\n",
    "    ax.tick_params(labelrotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 3, figsize=(4, 6.4))\n",
    "axes[0, 2].set_visible(False)\n",
    "axes[1, 2].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left column\n",
    "for i, key in enumerate(\n",
    "    [\"weekend_factor\", \"mu\", \"lambda_0\", \"lambda_1\", \"lambda_2\", \"lambda_3\"]\n",
    "):\n",
    "    cov19.plot._distribution(this_model, trace, key, ax=axes[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid column\n",
    "for i, key in enumerate(\n",
    "    [\n",
    "        \"offset_modulation\",\n",
    "        \"sigma_obs\",\n",
    "        \"I_begin\",\n",
    "        # beware, these guys were the begin of the transient in the paper,\n",
    "        # now they are the center points (shifted by transient_len_i)\n",
    "        \"transient_day_1\",\n",
    "        \"transient_day_2\",\n",
    "        \"transient_day_3\",\n",
    "    ]\n",
    "):\n",
    "    cov19.plot._distribution(this_model, trace, key, ax=axes[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right column\n",
    "for i, key in enumerate(\n",
    "    [\"delay\", \"transient_len_1\", \"transient_len_2\", \"transient_len_3\",]\n",
    "):\n",
    "    cov19.plot._distribution(this_model, trace, key, ax=axes[i + 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig #To show figure in jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
